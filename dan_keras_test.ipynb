{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from assets.keras.keras.models import Sequential\n",
    "from assets.keras.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from assets.keras.keras.layers.convolutional import Convolution3D, MaxPooling2D\n",
    "from assets.keras.keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (3, 100, 100) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution3D(32,3,3,3, input_shape=(1,32,32,16)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "# Note: Keras does automatic shape inference.\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load dataset from gen_dataset\n",
    "import cPickle as pickle\n",
    "data = pickle.load(open('assets/data/lr_ds.data','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (800, 1, 32, 32, 16)\n",
      "Train labels shape:  (800,)\n",
      "Validation data shape:  (100, 1, 32, 32, 16)\n",
      "Validation labels shape:  (100,)\n",
      "Test data shape:  (100, 1, 32, 32, 16)\n",
      "Test labels shape:  (100,)\n"
     ]
    }
   ],
   "source": [
    "X_train = data['X_train']\n",
    "X_train = np.expand_dims(X_train,axis=1)\n",
    "Y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "X_val = np.expand_dims(X_val,axis=1)\n",
    "Y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "X_test = np.expand_dims(X_test,axis=1)\n",
    "Y_test = data['y_test']\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', Y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', Y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train[0:200,:,:,:,:]\n",
    "Y_train = Y_train[0:200]\n",
    "X_test = X_test[0:40,:,:,:,:]\n",
    "Y_test = Y_test[0:40]\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train-1, 2)\n",
    "Y_test = np_utils.to_categorical(Y_test-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (200, 1, 32, 32, 16))\n",
      "('Y_train shape:', (200, 2))\n",
      "('X_test shape:', (40, 1, 32, 32, 16))\n",
      "('Y_test shape:', (40, 2))\n",
      "(200, 'train samples')\n",
      "(40, 'test samples')\n",
      "Train on 200 samples, validate on 40 samples\n",
      "Epoch 1/50\n",
      "25s - loss: 7.5953 - acc: 0.5250 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      "25s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      "25s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/50\n",
      "25s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/50\n",
      "25s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      "28s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/50\n",
      "24s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-af464f61fd3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2,\n\u001b[1;32m---> 91\u001b[1;33m           validation_data=(X_test, Y_test))\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/assignment2/motnet/assets/keras/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, show_accuracy, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m    644\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m                          \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m                          shuffle=shuffle, metrics=metrics)\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/assignment2/motnet/assets/keras/keras/models.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, metrics)\u001b[0m\n\u001b[0;32m    278\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/assignment2/motnet/assets/keras/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "__author__ = 'Minhaz Palasara'\n",
    "\n",
    "from assets.keras.keras.datasets import shapes_3d\n",
    "from assets.keras.keras.preprocessing.image import ImageDataGenerator\n",
    "from assets.keras.keras.models import Sequential\n",
    "from assets.keras.keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from assets.keras.keras.layers.convolutional import Convolution3D, MaxPooling3D, ZeroPadding3D\n",
    "from assets.keras.keras.optimizers import SGD, RMSprop\n",
    "from assets.keras.keras.utils import np_utils, generic_utils\n",
    "from assets.keras.keras.regularizers import l2\n",
    "import theano\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    To classify/track 3D shapes, such as human hands (http://www.dbs.ifi.lmu.de/~yu_k/icml2010_3dcnn.pdf),\n",
    "    we first need to find a distinct set of features. Specifically for 3D shapes, robust classification can be done using\n",
    "    3D features.\n",
    "\n",
    "    Features can be extracted by applying a 3D filters. We can auto learn these filters using 3D deep learning.\n",
    "\n",
    "    This example trains a simple network for classifying 3D shapes (Spheres, and Cubes).\n",
    "\n",
    "    GPU run command:\n",
    "        THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python shapes_3d_cnn.py\n",
    "\n",
    "    CPU run command:\n",
    "        THEANO_FLAGS=mode=FAST_RUN,device=cpu,floatX=float32 python shapes_3d_cnn.py\n",
    "\n",
    "    For 4000 training samples and 1000 test samples.\n",
    "    90% accuracy reached after 40 epochs, 37 seconds/epoch on GTX Titan\n",
    "\"\"\"\n",
    "\n",
    "# Data Generation parameters\n",
    "#test_split = 0.2\n",
    "#dataset_size = 100\n",
    "#patch_size = 16\n",
    "\n",
    "#(X_train, Y_train),(X_test, Y_test) = shapes_3d.load_data(test_split=test_split,\n",
    "#                                                          dataset_size=dataset_size,\n",
    "#                                                          patch_size=patch_size)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('Y_test shape:', Y_test.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# CNN Training parameters\n",
    "batch_size = 10\n",
    "nb_classes = 2\n",
    "nb_epoch = 50\n",
    "\n",
    "\n",
    "# number of convolutional filters to use at each layer\n",
    "nb_filters = [6, 6, 6]\n",
    "\n",
    "# level of pooling to perform at each layer (POOL x POOL)\n",
    "nb_pool = [3, 3, 3]\n",
    "\n",
    "# level of convolution to perform at each layer (CONV x CONV)\n",
    "nb_conv = [3, 3, 3]\n",
    "\n",
    "# Regularization\n",
    "reg = 0\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding3D((1,1,1),input_shape=(1,32,32,16)))\n",
    "model.add(Convolution3D(nb_filters[0],nb_depth=nb_conv[0], nb_row=nb_conv[0], nb_col=nb_conv[0], border_mode='valid',\n",
    "                         activation='relu', W_regularizer=l2(reg)))\n",
    "#model.add(MaxPooling3D(pool_size=(nb_pool[0], nb_pool[0], nb_pool[0])))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(ZeroPadding3D((1,1,1)))\n",
    "model.add(Convolution3D(nb_filters[1],nb_depth=nb_conv[1], nb_row=nb_conv[1], nb_col=nb_conv[1], border_mode='valid',\n",
    "                        activation='relu', W_regularizer=l2(reg)))\n",
    "#model.add(MaxPooling3D(pool_size=(nb_pool[1], nb_pool[1], nb_pool[1])))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(ZeroPadding3D((1,1,1)))\n",
    "model.add(Convolution3D(nb_filters[2],nb_depth=nb_conv[2], nb_row=nb_conv[2], nb_col=nb_conv[2], border_mode='valid',\n",
    "                        activation='relu', W_regularizer=l2(reg)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, init='normal', activation='relu', W_regularizer=l2(reg)))\n",
    "model.add(Dense(nb_classes, init='normal', W_regularizer=l2(reg)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = RMSprop(lr=.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, batch_size=batch_size, show_accuracy=True)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': [0.47999999999999998, 0.5, 0.46000000000000002, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], 'loss': [8.381409931182862, 8.0590476036071781, 8.6475760459899895, 8.0590476036071781, 8.0590477943420407, 8.0590478897094719, 8.0590476989746094, 8.0590478897094719, 8.0590476989746094, 8.0590477943420407], 'batch': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'val_acc': [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], 'val_loss': [7.5145745277404785, 7.5145745277404785, 8.0590476989746094, 8.0590476989746094, 8.0590476989746094, 8.0590476989746094, 8.0590476989746094, 8.0590476989746094, 8.0590476989746094, 8.0590476989746094], 'size': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]}\n"
     ]
    }
   ],
   "source": [
    "print hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255   0   0 ...,   0   0   0]\n",
      " [  0   0   0 ...,   0   0   0]\n",
      " [255   0   0 ...,   0   0   0]\n",
      " ..., \n",
      " [  0   0   0 ...,   0   0   0]\n",
      " [  0   0   0 ...,   0   0   0]\n",
      " [  0   0   0 ..., 255 255   0]]\n"
     ]
    }
   ],
   "source": [
    "print X_train[1,0,:,:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
